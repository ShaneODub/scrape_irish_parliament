# Create en empty list the hold the ads we're going to download
job_list <- vector("list", 99)
library(rvest)
irishjobs_url <- "https://www.irishjobs.ie/ShowResults.aspx?Keywords=Data%20Analyst&autosuggestEndpoint=/autosuggest&Location=102&Recruiter=Agency&btnSubmit=%20&PerPage=100"
# Create en empty list the hold the ads we're going to download
job_list <- vector("list", 99)
job_scrape <- read_html(irishjobs_url)
View(job_scrape)
html_nodes(job_list, css = ".job-result .module-content"
)
html_nodes(job_list[[1]], css = ".job-result .module-content")
str(job_list)
job_scrape
str(job_scrape)
View(job_scrape)
html_nodes(job_scrape[[1]], css = ".job-result .module-content")
html_nodes(job_scrape, css = ".job-result .module-content")
html_nodes(job_scrape, css = ".job-result .module-content")[1]
html_nodes(job_scrape, css = ".job-result .module-content")[1]
length(html_nodes(job_scrape, css = ".job-result .module-content"))
length(html_nodes(job_scrape, css = ".job-result p , .salary , .job-result-title a"))
html_nodes(job_scrape, css = ".job-result p , .salary , .job-result-title a")
html_nodes(job_scrape, css = ".job-result p , .salary , .job-result-title a")[1]
str(html_nodes(job_scrape, css = ".job-result p , .salary , .job-result-title a")[1])
str(html_nodes(job_scrape, css = ".job-result p , .salary , .job-result-title a")[2])
html_nodes(job_scrape, css = ".job-result p , .salary , .job-result-title a")[2]
html_nodes(job_scrape, css = ".job-result p , .salary , .job-result-title a")[3]
html_nodes(job_scrape, css = ".job-result p , .salary , .job-result-title a")[4]
html_nodes(job_scrape, css = ".job-result p , .salary , .job-result-title a")[1]
html_nodes(job_scrape, css = ".job-result p , .salary , .job-result-title a")[5]
html_nodes(job_scrape, css = ".job-result p , .salary , .job-result-title a")[6]
html_nodes(job_scrape, css = ".job-result p , .salary , .job-result-title a")[7]
html_nodes(job_scrape, css = ".job-result p , .salary , .job-result-title a")[8]
# Grab the recruiter, job title, salary, summary description from job_scrape
html_nodes(job_scrape, css = "h3 a")[1]
# Grab the recruiter, job title, salary, summary description from job_scrape
html_text(html_nodes(job_scrape, css = "h3 a")[1])
# Grab the recruiter, job title, salary, summary description from job_scrape
html_text(html_nodes(job_scrape, css = "h3 a")[2])
# Grab the recruiter, job title, salary, summary description from job_scrape
html_text(html_nodes(job_scrape, css = "h3 a")[3])
View(job_list)
rm(job_list)
extractedData <- tibble(job_list,
recruiter = character(99),
job_title = character(99),
salary = character(99),
description = character(99))
# library(stringr)
library(tidyverse)
extractedData <- tibble(job_list,
recruiter = character(99),
job_title = character(99),
salary = character(99),
description = character(99))
# library(stringr)
library(tidyr)
extractedData <- tibble(job_list,
recruiter = character(99),
job_title = character(99),
salary = character(99),
description = character(99))
extractedData <- tibble(job_scrape,
recruiter = character(99),
job_title = character(99),
salary = character(99),
description = character(99))
extractedData <- tibble(recruiter = character(99),
job_title = character(99),
salary = character(99),
description = character(99))
View(extractedData)
extractedData$recruiter <-
lapply(job_scrape, html_nodes, css = "h3 a") %>%
lapply(html_text) %>%
as.character()
View(job_scrape)
View(extractedData)
lapply(job_scrape, html_nodes, css = "h3 a")
str(job_scrape)
glimpse(job_scrape)
str(job_scrape)
i <- i + 1
i <- 1
while (i <= 99) {
extractedData$recruiter[i] <-
html_text(html_nodes(job_scrape, css = "h3 a")[i])
i <- i + 1
}
extractedData$description[i] <-
html_text(html_nodes(job_scrape, css = ".job-result p span")[i])
i <- 1
while (i <= 99) {
extractedData$recruiter[i] <-
html_text(html_nodes(job_scrape, css = "h3 a")[i])
extractedData$job_title[i] <-
html_text(html_nodes(job_scrape, css = "h2 a")[i])
extractedData$salary[i] <-
html_text(html_nodes(job_scrape, css = ".salary")[i])
extractedData$description[i] <-
html_text(html_nodes(job_scrape, css = ".job-result p span")[i])
i <- i + 1
}
View(extractedData)
library(ggplot2)
ggplot(data = extractedData, mapping = aes(x = recruiter)) + geom_bar()
ggplot(data = extractedData, mapping = aes(x = recruiter)) +
geom_bar() +
coord_flip()
mapping = aes(x = reorder(recruiter)) +
ggplot(data = extractedData,
mapping = aes(x = reorder(recruiter))) +
geom_bar() +
coord_flip()
ggplot(data = extractedData,
mapping = aes(x = recruiter)) +
geom_bar() +
coord_flip()
str(extractedData)
# Order the levels by number of jobs
extractedData$recruiter <-
group_by(recruiter) %>%
count()
library(dplyr)
# Order the levels by number of jobs
extractedData$recruiter <-
group_by(recruiter) %>%
count()
# Order the levels by number of jobs
extractedData <-
group_by(recruiter) %>%
count()
# Order the levels by number of jobs
extractedData <-
count()
# Order the levels by number of jobs
extractedData <-
count()
# Order the levels by number of jobs
extractedData <-
tally()
View(extractedData)
# Order the levels by number of jobs
extractedData %>%
group_by(recruiter) %>%
count()
# Order the levels by number of jobs
extractedData %>%
group_by(recruiter) %>%
count() %>%
order(decreasing = TRUE)
extractedData %>%
group_by(recruiter) %>%
count()
# Order the levels by number of jobs
recruiter_count <-
extractedData %>%
group_by(recruiter) %>%
count()
recruiter_count
View(recruiter_count)
# Order the levels by number of jobs
recruiter_count <-
extractedData %>%
group_by(recruiter) %>%
count() %>%
order(n)
recruiter_count[order(recruiter_count$n),]
recruiter_count[order(recruiter_count$n, decreasing = TRUE),]
extractedData %>%
group_by(recruiter) %>%
count() %>%
arrange(n)
# Order the levels by number of jobs
recruiter_count <-
extractedData %>%
group_by(recruiter) %>%
count() %>%
arrange(desc(n))
View(recruiter_count)
install.packages("forcats")
install.packages("forcats")
library(forcats)
str(extractedData)
ggplot(data = extractedData,
mapping = aes(x = recruiter) +
ggplot(data = extractedData,
mapping = aes(x = recruiter)) +
geom_bar() +
coord_flip()
library(ggplot2)
ggplot(data = extractedData,
mapping = aes(x = recruiter)) +
geom_bar() +
coord_flip()
extractedData$recruiter <-
fct_infreq(extractedData$recruiter)
str(extractedData)
ggplot(data = extractedData,
mapping = aes(x = recruiter)) +
geom_bar() +
coord_flip()
extractedData$recruiter <-
fct_infreq(-extractedData$recruiter)
ggplot(data = extractedData,
mapping = aes(x = recruiter)) +
geom_bar()
ggplot(data = extractedData,
mapping = aes(x = recruiter)) +
geom_bar() +
coord_flip()
extractedData$recruiter <-
fct_infreq(extractedData$recruiter) %>%
factor(levels = -levels)
extractedData$job_title <-
fct_infreq(extractedData$job_title))
extractedData$job_title <-
fct_infreq(extractedData$job_title)
ggplot(data = extractedData,
mapping = aes(x = job_title)) +
geom_bar() +
coord_flip()
rm(list = ls())
getwd()
setwd("C://Users//Shane//Onedrive//projects/taking_the_mick")
getwd()
getwd()
install.packages("pacman")
pacman::p_load(rvest)
url <- "https://www.oireachtas.ie/en/members/tds/?term=%2Fie%2Foireachtas%2Fhouse%2Fdail%2F"
test55 <- read_html(url)
View(test55)
html_text(html_nodes(test55, css = ".c-member-list-item")[1]
)
html_text(html_nodes(test55, css = ".c-member-list-item__name-content")[1])
term <- 33
page <- 1
max_page <- 8
pacman::p_load(rvest, dplyr)
# We'll store the pages after scraping
list.of.tibbles <- vector("list",1600)
# We'll store the pages after scraping
list_of_pages <- vector("list",80)
for (page in 1:9){
list.of.pages[[page + 9 * (term - 26)]] <-
paste0(url, term, '&tab=party&page=', page) %>%
read_html()
Sys.sleep(5)
}
rm(term)
rm(page)
rm(max_page)
# We'll store the pages after scraping
list_of_pages <- vector("list",80)
for (term in 26:27) {
for (page in 1:9){
list.of.pages[[page + 9 * (term - 26)]] <-
paste0(url, term, '&tab=party&page=', page) %>%
read_html()
Sys.sleep(5)
}
}
View(list_of_pages)
print(4)
print(page + 9 * (term - 26))
View(list_of_pages)
for (term in 26:27) {
for (page in 1:9){
list_of_pages[[page + 9 * (term - 26)]] <-
paste0(url, term, '&tab=party&page=', page) %>%
read_html()
Sys.sleep(5)
print(page + 9 * (term - 26))
}
}
View(list_of_pages)
list_of_pages[1]
html_text(html_nodes(list_of_pages[1], css = ".c-member-list-item__name-content")[1])
html_text(html_nodes(list_of_pages[[1]], css = ".c-member-list-item__name-content"))
# We'll store the pages after scraping
list_of_pages <- vector("list")
View(list_of_pages)
list_of_pages[[page + 9 * (term - 26)]] <-
paste0(url, term, '&tab=party&page=', page) %>%
read_html()
list_of_pages[[page + 9 * (term - 26)]] <-
paste0(url, term, '&tab=party&page=', page) %>%
read_html()
# We'll store the pages after scraping
list_of_pages <- vector("list", 200)
for (term in 26:27) {
for (page in 1:9){
list_of_pages[[page + 9 * (term - 26)]] <-
paste0(url, term, '&tab=party&page=', page) %>%
read_html()
Sys.sleep(5)
print(page + 9 * (term - 26))
}
}
View(list_of_pages)
# We'll store the pages after scraping
list_of_pages <- vector("list", 80)
View(list_of_pages)
for (page in 1:9){
list_of_pages[[page + 9 * (term - 26)]] <-
paste0(url, term, '&tab=party&page=', page) %>%
read_html()
Sys.sleep(5)
print(page + 9 * (term - 26))
}
View(list_of_pages)
html_text(html_nodes(list_of_pages[[10]], css = ".c-member-list-item__name-content"))
rm(page)
rm(term)
# We'll store the pages after scraping
list_of_pages <- vector("list", 80)
for (term in 26:33) {
for (page in 1:9){
list_of_pages[[page + 9 * (term - 26)]] <-
paste0(url, term, '&tab=party&page=', page) %>%
read_html()
Sys.sleep(5)
print(page + 9 * (term - 26))
}
}
View(list_of_pages)
html_text(html_nodes(list_of_pages[[10]], css = ".c-member-list-item__name-content"))
extracted_data <- tibble(member = character(1500),
constituency = factor(1500),
party = factor(1500))
extracted_data <- tibble(term = integer(1500),
member = character(1500),
constituency = character(1500),
party = character(1500))
View(extracted_data)
list_of_pages[[1]]
list_of_pages[[1]][1]
list_of_pages[[1]][[1]]
for (item in list_of_pages){html_text(html_nodes(item, css = '.c-member-list-item__name-content'))}
for (item in list_of_pages[[1]]){html_text(html_nodes(item, css = '.c-member-list-item__name-content'))}
for (item in list_of_pages){html_text(html_nodes(item, css = '.c-member-list-item__name-content')[[1]])}
for (item in list_of_pages){html_text(html_nodes(item, css = '.c-member-list-item__name-content'))[[1]]}
for (item in list_of_pages){print(item)}
html_text(html_nodes(list_of_pages[[1]], css = ".c-member-list-item__name-content"))
html_text(html_nodes(list_of_pages[[71]], css = ".c-member-list-item__name-content"))
html_text(html_nodes(list_of_pages[[1]], css = ".c-member-list-item__name-content"))
html_text(html_nodes(list_of_pages[[71]], css = ".c-member-list-item__name-content"))
# Members of current and previous parliaments are listed on this URL:
url <-
"https://www.oireachtas.ie/en/members/tds/?term=%2Fie%2Foireachtas%2Fhouse%2Fdail%2F"
# We'll store the pages after scraping
list_of_pages <- vector("list", 80)
View(list_of_pages)
# Members of current and previous parliaments are listed on this URL:
url <-
"https://www.oireachtas.ie/en/members/tds/?term=%2Fie%2Foireachtas%2Fhouse%2Fdail%2F"
# We'll store the pages after scraping
list_of_pages <- vector("list", 72)
rm(term)
rm(page)
rm(test55)
Sys.sleep(3)
for (term in 26:33) {
for (page in 1:9){
list_of_pages[[page + 9 * (term - 26)]] <-
paste0(url, term, '&tab=constituency&page=', page) %>%
read_html()
Sys.sleep(3)
print(page + 9 * (term - 26))
}
}
View(list_of_pages)
html_text(html_nodes(list_of_pages[[1]], css = "#constituency .c-member-list-item__name-content"))
html_text(html_nodes(list_of_pages[[2]], css = "#constituency .c-member-list-item__name-content"))
html_text(html_nodes(list_of_pages[[3]], css = "#constituency .c-member-list-item__name-content"))
length(list_of_pages[[3]])
html_text(html_nodes(list_of_pages[[3]], css = "#constituency .c-member-list-item__name-content"))[[1]]
extracted_data$member <-
html_text(html_nodes(list_of_pages[[page + 9 * (term - 26)]],
css = "#constituency .c-member-list-item__name-content"))[[td]]
for (term in 26:33) {
for (page in 1:9){
for (td in 1:20) {
extracted_data$term <- term
extracted_data$member <-
html_text(html_nodes(list_of_pages[[page + 9 * (term - 26)]],
css = "#constituency .c-member-list-item__name-content"))[[td]]
}
}
}
View(extracted_data)
for (term in 26:33) {
for (page in 1:9){
for (td in 1:20) {
extracted_data$term[[page + 9 * (term - 26) + td - 1]] <- term
extracted_data$member <-
html_text(html_nodes(list_of_pages[[page + 9 * (term - 26)]],
css = "#constituency .c-member-list-item__name-content"))[[td]]
}
}
}
View(extracted_data)
for (term in 26:33) {
for (page in 1:9){
for (td in 1:20) {
extracted_data$term[[page + 9 * (term - 26) + td - 1]] <- term
extracted_data$member[[page + 9 * (term - 26) + td - 1]] <-
html_text(html_nodes(list_of_pages[[page + 9 * (term - 26)]],
css = "#constituency .c-member-list-item__name-content"))[[td]]
}
}
}
list_of_pages[[1]],
css = "#constituency .c-member-list-item__name-content"))[[1]]
html_text(html_nodes(list_of_pages[[1]],
css = "#constituency .c-member-list-item__name-content"))[[1]]
html_text(html_nodes(list_of_pages[[2]],
css = "#constituency .c-member-list-item__name-content"))[[2]]
for (term in 26:33) {
for (page in 1:9){
for (td in 1:20) {
extracted_data$term[[20 * (page - 1) + 9 * (term - 26) + td]] <- term
extracted_data$member[[20 * (page - 1) + 9 * (term - 26) + td]] <-
html_text(html_nodes(list_of_pages[[page + 9 * (term - 26)]],
css = "#constituency .c-member-list-item__name-content"))[[td]]
}
}
}
